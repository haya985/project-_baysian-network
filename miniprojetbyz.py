# -*- coding: utf-8 -*-
"""miniprojetbyz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cdmJevbjiNVYIKJnJOh0k1nAc51SnOfE
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
from scipy.stats import randint

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.datasets import make_classification
from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier

from sklearn import metrics
from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve
from  sklearn.model_selection import cross_val_score

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import RandomizedSearchCV

from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier

pip install six

#Naive bayes
from sklearn.naive_bayes import GaussianNB 
#Stacking
from mlxtend.classifier import StackingClassifier

from google.colab import drive 
drive.mount('/content/gdrive')

#reading in CSV's from a file path
train_df = pd.read_csv('/content/gdrive/My Drive/data_adult.txt', sep="\t", encoding="utf-8")

testdf= pd.read_csv('/content/gdrive/My Drive/adult_test.txt', sep=",", encoding="utf-8")

print(train_df.shape)
testdf.isnull().sum().max() #just checking that there's no missing data missing...
testdf.head(5)

print(train_df.describe())

print(testdf.info())

testdf



train_df.isnull().sum().max() #just checking that there's no missing data missing...
train_df.head(5)

train_df.var()

for k in train_df.keys():
    print('{0}: {1}'.format(k, len(train_df[k].unique())))

for k in train_df.keys():
    if len(train_df[k].unique())<=15:
        print(k)



# Assign default values for each data type
defaultInt = 0
defaultString = 'NaN'
defaultFloat = 0.0

stringFeatures = ['education','workclass','marital_status','occupation','relationship','race','sex',"native_country","target"]

intFeatures = ['age','education_num','capital_gain','capital_loss','hours_per_week','fnlwgt']

floatFeatures = []

# Clean the NaN's
for feature in train_df:
    if feature in intFeatures:
        train_df[feature] = train_df[feature].fillna(defaultInt)
    elif feature in stringFeatures:
        train_df[feature] = train_df[feature].fillna(defaultString)
    elif feature in floatFeatures:
        train_df[feature] = train_df[feature].fillna(defaultFloat)
    else:
        print('Error: Feature %s not recognized.' % feature)
train_df.head(10)

pip install pyAgrum

import pandas
import os
import math
import pyAgrum as gum
import pyAgrum.lib.notebook as gnb
from pyAgrum.lib.bn2roc import showROC
from sklearn.tree import DecisionTreeClassifier
import sklearn.datasets as datasets
from sklearn.externals.six import StringIO
from sklearn.tree import export_graphviz

train_df['age'].fillna(train_df['age'].median(), inplace = True)

s = pd.Series(train_df['age'])
s[s<18] = train_df['age'].median()
train_df['age'] = s
s = pd.Series(train_df['age'])
s[s>90] = train_df['age'].median()
train_df['age'] = s

s = pd.Series(testdf['age'])
s[s<18] = testdf['age'].median()
testdf['age'] = s
s = pd.Series(testdf['age'])
s[s>90] = testdf['age'].median()
testdf['age'] = s

train_df['age_range'] = pd.cut(train_df['age'], [0,20,30,65,90], labels=["0-20", "21-30", "31-65", "66-90"], include_lowest=True)

testdf['age_range'] = pd.cut(testdf['age'], [0,20,30,65,90], labels=["0-20", "21-30", "31-65", "66-90"], include_lowest=True)

gender = train_df['sex'].str.lower()
print(gender)

gender = train_df['sex'].unique()
print(gender)

import csv
train_df.to_csv(os.path.join('/content/gdrive/My Drive/train_data2.csv'), index=False)

train_df.var()

for k in testdf.keys():
    if len(testdf[k].unique())<=15:
        print(k)

bn = gum.BayesNet("Adulte")
bn =gum.fastBN("age_range{0-20|21-30|31-65|66-90}<-target{<=50K|>50K}->sex{ Female| Male};relationship{ Wife| Own-child| Husband| Not-in-family| Other-relative| Unmarried}<-target->race{ White| Asian-Pac-Islander| Amer-Indian-Eskimo| Other| Black};workclass{ Private| Self-emp-not-inc| Self-emp-inc| Federal-gov| Local-gov| State-gov| Without-pay| Never-worked}<-target->marital_status{ Married-civ-spouse| Divorced| Never-married| Separated| Widowed| Married-spouse-absent| Married-AF-spouse};occupation{ Tech-support| Craft-repair| Other-service| Sales| Exec-managerial| Prof-specialty| Handlers-cleaners| Machine-op-inspct| Adm-clerical| Farming-fishing| Transport-moving| Priv-house-serv| Protective-serv| Armed-Forces}<-target")
print(bn.variable("target"))
print(bn.variable("age_range"))
print(bn.variable("sex"))
print(bn.variable("race"))
print(bn.variable("workclass"))
print(bn.variable("relationship"))
print(bn.variable("marital_status"))
print(bn.variable("occupation"))

bn

bn.cpt('target')[:] = [100, 1]
bn.cpt('target').normalizeAsCPT()
bn.cpt('target')

bn.cpt('sex')[0:] = [ 1, 10]
bn.cpt('sex')[1:] = [ 10, 1]
bn.cpt('sex').normalizeAsCPT()
bn.cpt('sex')

bn.cpt('age_range')[0:] = [ 1, 1, 10, 10]
bn.cpt('age_range')[1:] = [ 10, 10, 1, 1]
bn.cpt('age_range').normalizeAsCPT()
bn.cpt('age_range')

bn.cpt('race')[0:] = [ 1, 1,1,10, 10]
bn.cpt('race')[1:] = [ 10, 10,1,1, 1]
bn.cpt('race').normalizeAsCPT()
bn.cpt('race')

bn.cpt('workclass')[0:] = [ 1, 1, 1, 1,10,10,10,10]
bn.cpt('workclass')[1:] = [ 10, 10, 10, 10,1,1,1,1]
bn.cpt('workclass').normalizeAsCPT()
bn.cpt('workclass')

bn.cpt('relationship')[0:] = [ 1, 1, 1,10,10,10]
bn.cpt('relationship')[1:] = [ 10, 10, 10,1,1,1]
bn.cpt('relationship').normalizeAsCPT()
bn.cpt('relationship')

bn.cpt('marital_status')[0:] = [ 1, 1, 1,1,10,10,10]
bn.cpt('marital_status')[1:] = [ 10, 10, 10,10,1,1,1]
bn.cpt('marital_status').normalizeAsCPT()
bn.cpt('marital_status')

bn.cpt('occupation')[0:] = [ 1, 1, 1,1,1,1,1,10,10,10,10,10,10,10]
bn.cpt('occupation')[1:] = [ 10, 10,10,10,10,10,10,1,1,1,1,1,1,1]
bn.cpt('occupation').normalizeAsCPT()
bn.cpt('occupation')

gnb.showInference(bn,size="10")

gnb.showInference(bn,size="10", evs={'target':'<=50K'})
gnb.showInference(bn,size="10", evs={'target':'>50K'})

gnb.showInference(bn,size="10", evs={'target':'<=50K', 'sex':' Male'})

male_str=[" ?"]
for (row, col) in train_df.iterrows():

    if str.lower(col.occupation) in male_str:
        train_df['occupation'].replace(to_replace=col.occupation, value=' Sales', inplace=True)
    if str.lower(col.workclass) in male_str:
        train_df['workclass'].replace(to_replace=col.workclass, value=' Private', inplace=True)

ie=gum.LazyPropagation(bn)

def init_belief(engine):
    # Initialize evidence
    for var in engine.BN().names():
        if var != 'target':
            engine.addEvidence(var, 0)

def update_beliefs(engine, bayesNet, row):
    # Update beliefs from a given row less the Survived variable
    for var in bayesNet.names():
        if var == "target":
            continue
        try:
            label = str(row.to_dict()[var])
            idx = bayesNet.variable(var).index(str(row.to_dict()[var]))
            engine.chgEvidence(var, idx)
        except gum.NotFound:
            # this can happend when value is missing is the test base.
            pass        
    engine.makeInference()
    
def is_well_predicted(engine, bayesNet, auc, row):
    update_beliefs(engine, bayesNet, row)
    marginal = engine.posterior('target')
    outcome = row.to_dict()['target']
    if outcome == "False": # Did not survived
        if marginal.toarray()[1] < auc:
            return "True Positive"
        else:
            return "False Negative"
    else: # Survived
        if marginal.toarray()[1] >= auc:
            return "True Negative"
        else:
            return "False Positive"

init_belief(ie)
ie.addTarget('target')
result = train_df.apply(lambda x: is_well_predicted(ie, bn, 0.5, x), axis=1)
result.value_counts(True)

positives = sum(result.map(lambda x: 1 if x.startswith("True") else 0 ))
total = result.count()
print("{0:.2f}% good predictions".format(positives/total*100))

df = pandas.read_csv(os.path.join('res', 'titanic', '/content/gdrive/My Drive/train_data2.csv'))

for k in train_df.keys():
    print('{0}: {1}'.format(k, len(train_df[k].unique())))

template=gum.BayesNet()
template.add(gum.LabelizedVariable("target", "target", ['<=50K', '>50K']))
template.add(gum.LabelizedVariable("sex", "sex",['Male','Female']))
template.add(gum.LabelizedVariable("age_range", "age_range",['0-20','21-30','31-65','66-90']))
template.add(gum.LabelizedVariable("race", "race",['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black']))
template.add(gum.LabelizedVariable("workclass", "workclass",['Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay', 'Never-worked']))
template.add(gum.LabelizedVariable("relationship", "relationship", ['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried']))
template.add(gum.LabelizedVariable("marital_status", "marital_status", ['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'])) 
template.add(gum.LabelizedVariable("occupation", "occupation",['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty', 'Handlers-cleaners', 'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv', 'Protective-serv', 'Armed-Forces']))            
gnb.showBN(template)

train_df.to_csv(os.path.join('/content/gdrive/My Drive/train_data2.csv'), index=False)
file = os.path.join('res', 'titanic', '/content/gdrive/My Drive/train_data2.csv')

learner = gum.BNLearner(file, template)
bn = learner.learnBN()
bn

gnb.showInformation(bn,{},size="20")

gnb.showInference(bn)

gnb.showPosterior(bn,evs={"sex": "Male", "age_range": '21-30'},target='target')

gnb.sideBySide(bn, gum.MarkovBlanket(bn, 'target'), captions=["Learned Bayesian Network", "Markov blanket of 'target'"])



ie=gum.LazyPropagation(bn)
init_belief(ie)
ie.addTarget('target')
result = testdf.apply(lambda x: is_well_predicted(ie, bn, 0.157935, x), axis=1)
result.value_counts(True)

positives = sum(result.map(lambda x: 1 if x.startswith("True") else 0 ))
total = result.count()
print("{0:.2f}% good predictions".format(positives/total*100))

showROC(bn,file, 'target', "True", True, True)